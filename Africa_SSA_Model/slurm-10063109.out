None

       optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)



        max_epoch = 12
        validation = 3


    
MONAI version: 1.2.0rc7+9.g84940298
Numpy version: 1.24.2
Pytorch version: 1.13.1
MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False
MONAI rev id: 84940298bff9ff1f54845eb8ae7422c0092c09c1
MONAI __file__: /home/guest185/hackathon_38/lib/python3.8/site-packages/monai/__init__.py

Optional dependencies:
Pytorch Ignite version: 0.4.2
ITK version: NOT INSTALLED or UNKNOWN VERSION.
Nibabel version: 5.1.0
scikit-image version: 0.19.3
Pillow version: 9.5.0
Tensorboard version: 2.13.0
gdown version: 4.7.1
TorchVision version: 0.14.1
tqdm version: 4.65.0
lmdb version: 1.2.1
psutil version: 5.9.4
pandas version: 1.5.3
einops version: 0.6.1
transformers version: 4.21.3
mlflow version: NOT INSTALLED or UNKNOWN VERSION.
pynrrd version: 1.0.0

For details about installing the optional dependencies, please visit:
    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies

/tmp/tmpg4j76b4p
image shape: (240, 240, 155), label shape: (240, 240, 155)
Thu Aug 10 22:57:28 2023 Epoch: 0
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
Epoch 0/12 0/48 loss: 0.9556 time 27.08s
Epoch 0/12 1/48 loss: 0.9401 time 4.28s
Epoch 0/12 2/48 loss: 0.9064 time 4.61s
Epoch 0/12 3/48 loss: 0.9171 time 4.21s
Epoch 0/12 4/48 loss: 0.9004 time 4.23s
Epoch 0/12 5/48 loss: 0.9103 time 4.25s
Epoch 0/12 6/48 loss: 0.9087 time 4.25s
Epoch 0/12 7/48 loss: 0.9028 time 4.26s
Epoch 0/12 8/48 loss: 0.9135 time 4.28s
Epoch 0/12 9/48 loss: 0.9126 time 4.30s
Epoch 0/12 10/48 loss: 0.9174 time 4.31s
Epoch 0/12 11/48 loss: 0.9208 time 4.30s
Epoch 0/12 12/48 loss: 0.9239 time 4.31s
Epoch 0/12 13/48 loss: 0.9186 time 4.31s
Epoch 0/12 14/48 loss: 0.9212 time 4.34s
Epoch 0/12 15/48 loss: 0.9179 time 4.34s
Epoch 0/12 16/48 loss: 0.9187 time 4.35s
Epoch 0/12 17/48 loss: 0.9167 time 4.36s
Epoch 0/12 18/48 loss: 0.9175 time 4.37s
Epoch 0/12 19/48 loss: 0.9144 time 4.38s
Epoch 0/12 20/48 loss: 0.9130 time 4.38s
Epoch 0/12 21/48 loss: 0.9109 time 4.38s
Epoch 0/12 22/48 loss: 0.9085 time 4.38s
Epoch 0/12 23/48 loss: 0.9057 time 4.38s
Epoch 0/12 24/48 loss: 0.9069 time 4.38s
Epoch 0/12 25/48 loss: 0.9080 time 4.39s
Epoch 0/12 26/48 loss: 0.9085 time 4.38s
Epoch 0/12 27/48 loss: 0.9109 time 4.38s
Epoch 0/12 28/48 loss: 0.9097 time 4.39s
Epoch 0/12 29/48 loss: 0.9081 time 4.38s
Epoch 0/12 30/48 loss: 0.9063 time 4.37s
Epoch 0/12 31/48 loss: 0.9050 time 4.38s
Epoch 0/12 32/48 loss: 0.9037 time 4.37s
Epoch 0/12 33/48 loss: 0.9046 time 4.37s
Epoch 0/12 34/48 loss: 0.9033 time 4.37s
Epoch 0/12 35/48 loss: 0.9006 time 4.37s
Epoch 0/12 36/48 loss: 0.9007 time 4.38s
Epoch 0/12 37/48 loss: 0.9009 time 4.38s
Epoch 0/12 38/48 loss: 0.8993 time 4.38s
Epoch 0/12 39/48 loss: 0.8975 time 4.37s
Epoch 0/12 40/48 loss: 0.8990 time 4.37s
Epoch 0/12 41/48 loss: 0.8961 time 4.37s
Epoch 0/12 42/48 loss: 0.8956 time 4.37s
Epoch 0/12 43/48 loss: 0.8974 time 4.37s
Epoch 0/12 44/48 loss: 0.8978 time 4.36s
Epoch 0/12 45/48 loss: 0.8996 time 4.35s
Epoch 0/12 46/48 loss: 0.8986 time 4.35s
Epoch 0/12 47/48 loss: 0.8994 time 4.35s
Final training  0/11 loss: 0.8994 time 231.69s
None of the inputs have requires_grad=True. Gradients will be None
Val 0/12 0/12 , dice_tc: 0.28055292 , dice_wt: 0.4399451 , dice_et: 0.012273716 , time 46.16s
Traceback (most recent call last):
  File "/home/guest185/hackathon_38/lib/python3.8/site-packages/nibabel/analyze.py", line 569, in set_data_dtype
    dt = np.dtype(dt)
TypeError: Cannot interpret 'torch.float32' as a data type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "ssa_swin_model.py", line 425, in <module>
    ) = trainer(
  File "ssa_swin_model.py", line 363, in trainer
    val_acc = val_epoch(
  File "ssa_swin_model.py", line 316, in val_epoch
    generate_segmentation_nifti(val_output, case_id, timepoint, output_dir)
  File "/home/guest185/SPARK_Stater/Africa_SSA_Model/segmentation_generate.py", line 12, in generate_segmentation_nifti
    segmentation_nifti = nib.Nifti1Image(segmentation_data, affine=np.eye(4))
  File "/home/guest185/hackathon_38/lib/python3.8/site-packages/nibabel/nifti1.py", line 1848, in __init__
    super().__init__(dataobj, affine, header, extra, file_map, dtype)
  File "/home/guest185/hackathon_38/lib/python3.8/site-packages/nibabel/analyze.py", line 909, in __init__
    super().__init__(dataobj, affine, header, extra, file_map)
  File "/home/guest185/hackathon_38/lib/python3.8/site-packages/nibabel/spatialimages.py", line 529, in __init__
    self._header.set_data_dtype(dataobj.dtype)
  File "/home/guest185/hackathon_38/lib/python3.8/site-packages/nibabel/nifti1.py", line 958, in set_data_dtype
    super().set_data_dtype(datatype)
  File "/home/guest185/hackathon_38/lib/python3.8/site-packages/nibabel/analyze.py", line 571, in set_data_dtype
    raise HeaderDataError(f'data dtype "{datatype}" not recognized')
nibabel.spatialimages.HeaderDataError: data dtype "torch.float32" not recognized
