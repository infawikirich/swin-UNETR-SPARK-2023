None

        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)



        max_epoch =  12
        validation = 4

    
MONAI version: 1.2.0rc7+9.g84940298
Numpy version: 1.24.2
Pytorch version: 1.13.1
MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False
MONAI rev id: 84940298bff9ff1f54845eb8ae7422c0092c09c1
MONAI __file__: /home/guest185/hackathon_38/lib/python3.8/site-packages/monai/__init__.py

Optional dependencies:
Pytorch Ignite version: 0.4.2
ITK version: NOT INSTALLED or UNKNOWN VERSION.
Nibabel version: 5.1.0
scikit-image version: 0.19.3
Pillow version: 9.5.0
Tensorboard version: 2.13.0
gdown version: 4.7.1
TorchVision version: 0.14.1
tqdm version: 4.65.0
lmdb version: 1.2.1
psutil version: 5.9.4
pandas version: 1.5.3
einops version: 0.6.1
transformers version: 4.21.3
mlflow version: NOT INSTALLED or UNKNOWN VERSION.
pynrrd version: 1.0.0

For details about installing the optional dependencies, please visit:
    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies

/tmp/tmp5i4_45a2
image shape: (240, 240, 155), label shape: (240, 240, 155)
Sun Aug 13 09:57:11 2023 Epoch: 0
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
In the future `np.bool` will be defined as the corresponding NumPy scalar.
Epoch 0/12 0/48 loss: 0.8772 time 31.35s
Epoch 0/12 1/48 loss: 0.8898 time 4.19s
Epoch 0/12 2/48 loss: 0.8892 time 4.54s
Epoch 0/12 3/48 loss: 0.9058 time 4.13s
Epoch 0/12 4/48 loss: 0.9054 time 4.15s
Epoch 0/12 5/48 loss: 0.9041 time 4.16s
Epoch 0/12 6/48 loss: 0.8989 time 4.15s
Epoch 0/12 7/48 loss: 0.9040 time 4.17s
Epoch 0/12 8/48 loss: 0.9117 time 4.17s
Epoch 0/12 9/48 loss: 0.9134 time 4.18s
Epoch 0/12 10/48 loss: 0.9053 time 4.19s
Epoch 0/12 11/48 loss: 0.9039 time 4.20s
Epoch 0/12 12/48 loss: 0.9038 time 4.20s
Epoch 0/12 13/48 loss: 0.9069 time 4.20s
Epoch 0/12 14/48 loss: 0.9033 time 4.21s
Epoch 0/12 15/48 loss: 0.9084 time 4.21s
Epoch 0/12 16/48 loss: 0.9108 time 4.22s
Epoch 0/12 17/48 loss: 0.9132 time 4.22s
Epoch 0/12 18/48 loss: 0.9162 time 4.23s
Epoch 0/12 19/48 loss: 0.9170 time 4.24s
Epoch 0/12 20/48 loss: 0.9201 time 4.22s
Epoch 0/12 21/48 loss: 0.9191 time 4.24s
Epoch 0/12 22/48 loss: 0.9200 time 4.25s
Epoch 0/12 23/48 loss: 0.9195 time 4.25s
Epoch 0/12 24/48 loss: 0.9214 time 4.25s
Epoch 0/12 25/48 loss: 0.9218 time 4.26s
Epoch 0/12 26/48 loss: 0.9230 time 4.25s
Epoch 0/12 27/48 loss: 0.9219 time 4.27s
Epoch 0/12 28/48 loss: 0.9201 time 4.26s
Epoch 0/12 29/48 loss: 0.9166 time 4.27s
Epoch 0/12 30/48 loss: 0.9155 time 4.27s
Epoch 0/12 31/48 loss: 0.9141 time 4.27s
Epoch 0/12 32/48 loss: 0.9161 time 4.27s
Epoch 0/12 33/48 loss: 0.9174 time 4.27s
Epoch 0/12 34/48 loss: 0.9160 time 4.27s
Epoch 0/12 35/48 loss: 0.9149 time 4.28s
Epoch 0/12 36/48 loss: 0.9146 time 4.28s
Epoch 0/12 37/48 loss: 0.9126 time 4.29s
Epoch 0/12 38/48 loss: 0.9131 time 4.30s
Epoch 0/12 39/48 loss: 0.9124 time 4.29s
Epoch 0/12 40/48 loss: 0.9096 time 4.29s
Epoch 0/12 41/48 loss: 0.9106 time 4.29s
Epoch 0/12 42/48 loss: 0.9110 time 4.29s
Epoch 0/12 43/48 loss: 0.9120 time 4.29s
Epoch 0/12 44/48 loss: 0.9139 time 4.29s
Epoch 0/12 45/48 loss: 0.9145 time 4.29s
Epoch 0/12 46/48 loss: 0.9133 time 4.29s
Epoch 0/12 47/48 loss: 0.9124 time 4.29s
Final training  0/11 loss: 0.9124 time 231.03s
Val 0/12 0/12 , dice_tc: 0.0011356008 , dice_wt: 0.3295844 , dice_et: 0.04279929 , time 62.35s
Val 0/12 1/12 , dice_tc: 0.0015611823 , dice_wt: 0.3040111 , dice_et: 0.032965686 , time 34.28s
Val 0/12 2/12 , dice_tc: 0.0047955103 , dice_wt: 0.3274977 , dice_et: 0.04428069 , time 34.20s
Val 0/12 3/12 , dice_tc: 0.0039072386 , dice_wt: 0.3186032 , dice_et: 0.040577512 , time 34.14s
Val 0/12 4/12 , dice_tc: 0.0031364257 , dice_wt: 0.2618481 , dice_et: 0.034218173 , time 34.10s
Val 0/12 5/12 , dice_tc: 0.0026222197 , dice_wt: 0.22644038 , dice_et: 0.028824827 , time 33.95s
Val 0/12 6/12 , dice_tc: 0.0022781598 , dice_wt: 0.19882181 , dice_et: 0.025981767 , time 33.85s
Val 0/12 7/12 , dice_tc: 0.0028385876 , dice_wt: 0.21106614 , dice_et: 0.0360183 , time 33.83s
Val 0/12 8/12 , dice_tc: 0.002952284 , dice_wt: 0.22163562 , dice_et: 0.036951263 , time 33.75s
Val 0/12 9/12 , dice_tc: 0.0031390705 , dice_wt: 0.23335405 , dice_et: 0.040348195 , time 33.77s
Val 0/12 10/12 , dice_tc: 0.0033533738 , dice_wt: 0.23374406 , dice_et: 0.03937833 , time 33.72s
Val 0/12 11/12 , dice_tc: 0.0031041645 , dice_wt: 0.22060283 , dice_et: 0.037040714 , time 33.68s
Final validation stats 0/11 , dice_tc: 0.0031041645 , dice_wt: 0.22060283 , dice_et: 0.037040714 , Dice_Avg: 0.0869159 , time 436.49s
new best (0.000000 --> 0.086916). 
Saving checkpoint /tmp/tmp5i4_45a2/model.pt
Sun Aug 13 10:08:19 2023 Epoch: 1
None of the inputs have requires_grad=True. Gradients will be None
Epoch 1/12 0/48 loss: 0.9597 time 10.96s
Epoch 1/12 1/48 loss: 0.8873 time 4.24s
Epoch 1/12 2/48 loss: 0.8642 time 4.25s
Epoch 1/12 3/48 loss: 0.8814 time 4.23s
Epoch 1/12 4/48 loss: 0.8927 time 4.25s
Epoch 1/12 5/48 loss: 0.8887 time 4.24s
Epoch 1/12 6/48 loss: 0.9045 time 4.25s
Epoch 1/12 7/48 loss: 0.9013 time 4.25s
Epoch 1/12 8/48 loss: 0.9004 time 4.24s
Epoch 1/12 9/48 loss: 0.8958 time 4.25s
Epoch 1/12 10/48 loss: 0.8918 time 4.25s
Epoch 1/12 11/48 loss: 0.8890 time 4.25s
Epoch 1/12 12/48 loss: 0.8939 time 4.25s
Epoch 1/12 13/48 loss: 0.8906 time 4.25s
Epoch 1/12 14/48 loss: 0.8879 time 4.25s
Epoch 1/12 15/48 loss: 0.8860 time 4.25s
Epoch 1/12 16/48 loss: 0.8914 time 4.25s
Epoch 1/12 17/48 loss: 0.8897 time 4.25s
Epoch 1/12 18/48 loss: 0.8877 time 4.26s
Epoch 1/12 19/48 loss: 0.8906 time 4.25s
Epoch 1/12 20/48 loss: 0.8939 time 4.25s
Epoch 1/12 21/48 loss: 0.8957 time 4.25s
Epoch 1/12 22/48 loss: 0.8933 time 4.26s
Epoch 1/12 23/48 loss: 0.8911 time 4.26s
Epoch 1/12 24/48 loss: 0.8887 time 4.26s
Epoch 1/12 25/48 loss: 0.8870 time 4.26s
Epoch 1/12 26/48 loss: 0.8897 time 4.25s
Epoch 1/12 27/48 loss: 0.8909 time 4.26s
Epoch 1/12 28/48 loss: 0.8900 time 4.26s
Epoch 1/12 29/48 loss: 0.8872 time 4.26s
Epoch 1/12 30/48 loss: 0.8886 time 4.25s
Epoch 1/12 31/48 loss: 0.8893 time 4.25s
Epoch 1/12 32/48 loss: 0.8853 time 4.25s
Epoch 1/12 33/48 loss: 0.8854 time 4.25s
Epoch 1/12 34/48 loss: 0.8866 time 4.25s
Epoch 1/12 35/48 loss: 0.8836 time 4.26s
Epoch 1/12 36/48 loss: 0.8851 time 4.25s
Epoch 1/12 37/48 loss: 0.8847 time 4.27s
Epoch 1/12 38/48 loss: 0.8852 time 4.25s
Epoch 1/12 39/48 loss: 0.8875 time 4.25s
Epoch 1/12 40/48 loss: 0.8889 time 4.26s
Epoch 1/12 41/48 loss: 0.8856 time 4.26s
Epoch 1/12 42/48 loss: 0.8877 time 4.25s
Epoch 1/12 43/48 loss: 0.8882 time 4.26s
Epoch 1/12 44/48 loss: 0.8863 time 4.26s
Epoch 1/12 45/48 loss: 0.8834 time 4.26s
Epoch 1/12 46/48 loss: 0.8838 time 4.25s
Epoch 1/12 47/48 loss: 0.8844 time 4.26s
Final training  1/11 loss: 0.8844 time 210.93s
Sun Aug 13 10:11:50 2023 Epoch: 2
Epoch 2/12 0/48 loss: 0.9245 time 8.32s
Epoch 2/12 1/48 loss: 0.8871 time 4.24s
Epoch 2/12 2/48 loss: 0.8936 time 4.24s
Epoch 2/12 3/48 loss: 0.9145 time 4.25s
Epoch 2/12 4/48 loss: 0.8809 time 4.25s
Epoch 2/12 5/48 loss: 0.8738 time 4.25s
Epoch 2/12 6/48 loss: 0.8681 time 4.25s
Epoch 2/12 7/48 loss: 0.8748 time 4.25s
Epoch 2/12 8/48 loss: 0.8594 time 4.26s
Epoch 2/12 9/48 loss: 0.8665 time 4.26s
Epoch 2/12 10/48 loss: 0.8693 time 4.26s
Epoch 2/12 11/48 loss: 0.8801 time 4.25s
Epoch 2/12 12/48 loss: 0.8752 time 4.26s
Epoch 2/12 13/48 loss: 0.8786 time 4.26s
Epoch 2/12 14/48 loss: 0.8739 time 4.26s
Epoch 2/12 15/48 loss: 0.8790 time 4.25s
Epoch 2/12 16/48 loss: 0.8787 time 4.25s
Epoch 2/12 17/48 loss: 0.8775 time 4.25s
Epoch 2/12 18/48 loss: 0.8749 time 4.25s
Epoch 2/12 19/48 loss: 0.8742 time 4.25s
Epoch 2/12 20/48 loss: 0.8753 time 4.25s
Epoch 2/12 21/48 loss: 0.8783 time 4.25s
Epoch 2/12 22/48 loss: 0.8738 time 4.25s
Epoch 2/12 23/48 loss: 0.8769 time 4.25s
Epoch 2/12 24/48 loss: 0.8745 time 4.26s
Epoch 2/12 25/48 loss: 0.8726 time 4.26s
Epoch 2/12 26/48 loss: 0.8718 time 4.26s
Epoch 2/12 27/48 loss: 0.8736 time 4.26s
Epoch 2/12 28/48 loss: 0.8715 time 4.26s
Epoch 2/12 29/48 loss: 0.8750 time 4.25s
Epoch 2/12 30/48 loss: 0.8731 time 4.25s
Epoch 2/12 31/48 loss: 0.8763 time 4.25s
Epoch 2/12 32/48 loss: 0.8744 time 4.25s
Epoch 2/12 33/48 loss: 0.8752 time 4.25s
Epoch 2/12 34/48 loss: 0.8771 time 4.25s
Epoch 2/12 35/48 loss: 0.8745 time 4.25s
Epoch 2/12 36/48 loss: 0.8732 time 4.25s
Epoch 2/12 37/48 loss: 0.8749 time 4.25s
Epoch 2/12 38/48 loss: 0.8718 time 4.25s
Epoch 2/12 39/48 loss: 0.8733 time 4.25s
Epoch 2/12 40/48 loss: 0.8719 time 4.25s
Epoch 2/12 41/48 loss: 0.8721 time 4.25s
Epoch 2/12 42/48 loss: 0.8694 time 4.26s
Epoch 2/12 43/48 loss: 0.8714 time 4.25s
Epoch 2/12 44/48 loss: 0.8721 time 4.25s
Epoch 2/12 45/48 loss: 0.8688 time 4.25s
Epoch 2/12 46/48 loss: 0.8668 time 4.25s
Epoch 2/12 47/48 loss: 0.8678 time 4.25s
Final training  2/11 loss: 0.8678 time 208.30s
Sun Aug 13 10:15:18 2023 Epoch: 3
Epoch 3/12 0/48 loss: 0.8995 time 7.93s
Epoch 3/12 1/48 loss: 0.8541 time 4.24s
Epoch 3/12 2/48 loss: 0.8782 time 4.24s
Epoch 3/12 3/48 loss: 0.8610 time 4.24s
Epoch 3/12 4/48 loss: 0.8886 time 4.25s
Epoch 3/12 5/48 loss: 0.8663 time 4.25s
Epoch 3/12 6/48 loss: 0.8696 time 4.25s
Epoch 3/12 7/48 loss: 0.8502 time 4.26s
Epoch 3/12 8/48 loss: 0.8491 time 4.25s
Epoch 3/12 9/48 loss: 0.8401 time 4.26s
Epoch 3/12 10/48 loss: 0.8338 time 4.26s
Epoch 3/12 11/48 loss: 0.8321 time 4.25s
Epoch 3/12 12/48 loss: 0.8409 time 4.25s
Epoch 3/12 13/48 loss: 0.8450 time 4.25s
Epoch 3/12 14/48 loss: 0.8443 time 4.25s
Epoch 3/12 15/48 loss: 0.8475 time 4.25s
Epoch 3/12 16/48 loss: 0.8549 time 4.26s
Epoch 3/12 17/48 loss: 0.8519 time 4.25s
Epoch 3/12 18/48 loss: 0.8489 time 4.25s
Epoch 3/12 19/48 loss: 0.8487 time 4.25s
Epoch 3/12 20/48 loss: 0.8425 time 4.25s
Epoch 3/12 21/48 loss: 0.8413 time 4.25s
Epoch 3/12 22/48 loss: 0.8445 time 4.25s
Epoch 3/12 23/48 loss: 0.8402 time 4.24s
Epoch 3/12 24/48 loss: 0.8374 time 4.27s
Epoch 3/12 25/48 loss: 0.8402 time 4.26s
Epoch 3/12 26/48 loss: 0.8444 time 4.25s
Epoch 3/12 27/48 loss: 0.8450 time 4.25s
Epoch 3/12 28/48 loss: 0.8431 time 4.25s
Epoch 3/12 29/48 loss: 0.8474 time 4.25s
Epoch 3/12 30/48 loss: 0.8496 time 4.25s
Epoch 3/12 31/48 loss: 0.8524 time 4.25s
Epoch 3/12 32/48 loss: 0.8548 time 4.25s
Epoch 3/12 33/48 loss: 0.8544 time 4.25s
Epoch 3/12 34/48 loss: 0.8503 time 4.25s
Epoch 3/12 35/48 loss: 0.8508 time 4.25s
Epoch 3/12 36/48 loss: 0.8535 time 4.25s
Epoch 3/12 37/48 loss: 0.8550 time 4.25s
Epoch 3/12 38/48 loss: 0.8539 time 4.26s
Epoch 3/12 39/48 loss: 0.8525 time 4.27s
Epoch 3/12 40/48 loss: 0.8523 time 4.26s
Epoch 3/12 41/48 loss: 0.8513 time 4.26s
Epoch 3/12 42/48 loss: 0.8542 time 4.26s
Epoch 3/12 43/48 loss: 0.8550 time 4.25s
Epoch 3/12 44/48 loss: 0.8538 time 4.26s
Epoch 3/12 45/48 loss: 0.8551 time 4.26s
Epoch 3/12 46/48 loss: 0.8568 time 4.26s
Epoch 3/12 47/48 loss: 0.8582 time 4.25s
Final training  3/11 loss: 0.8582 time 207.94s
